{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd().split('render')[0],'gaussian_data'))\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from gaussian_data.Camera import Camera\n",
    "from gaussian_data.Frame import Frame\n",
    "import gaussian_data.Plotters\n",
    "import gaussian_data.Utils as Utils\n",
    "from GaussianSplat import GaussianSplat\n",
    "import numpy as np\n",
    "from plyfile import PlyData\n",
    "import numpy as np\n",
    "import argparse\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "from Render import Render\n",
    "\n",
    "%matplotlib qt\n",
    "\n",
    "# load hull\n",
    "path = 'I:/My Drive/Research/gs_data/mov19_2022_03_03/'\n",
    "real_coord = scipy.io.loadmat(f'{path}/3d_pts/real_coord.mat')['all_coords']\n",
    "points_3d = {body_wing : pd.DataFrame(Utils.load_hull(body_wing,path),columns = ['X','Y','Z','frame']) for body_wing in ['body','rwing','lwing']}\n",
    "# initilize objects\n",
    "frames = [1483]\n",
    "\n",
    "image_names,points_in_idx = Utils.define_frames(frames,points_3d)\n",
    "cameras = {f'cam{cam + 1}':Camera(path,cam) for cam in range(4)}\n",
    "frames = {f'{im_name}.jpg':Frame(path,im_name,points_in_idx[im_name.split('CAM')[0]],real_coord,idx) for idx,im_name in enumerate(image_names)}\n",
    "# map 3d voxels to 2d pixels\n",
    "[frames[im_name].map_3d_2d(croped_image = True) for im_name in frames.keys()]\n",
    "voxel_dict,colors_dict = Utils.get_dict_for_points3d(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rotation(r):\n",
    "    norm = np.sqrt(r[:,0]*r[:,0] + r[:,1]*r[:,1] + r[:,2]*r[:,2] + r[:,3]*r[:,3])\n",
    "\n",
    "    q = r / norm[:, None]\n",
    "\n",
    "    R = np.zeros((q.shape[0], 3, 3))\n",
    "\n",
    "    r = q[:, 0]\n",
    "    x = q[:, 1]\n",
    "    y = q[:, 2]\n",
    "    z = q[:, 3]\n",
    "\n",
    "    R[:, 0, 0] = 1 - 2 * (y*y + z*z)\n",
    "    R[:, 0, 1] = 2 * (x*y - r*z)\n",
    "    R[:, 0, 2] = 2 * (x*z + r*y)\n",
    "    R[:, 1, 0] = 2 * (x*y + r*z)\n",
    "    R[:, 1, 1] = 1 - 2 * (x*x + z*z)\n",
    "    R[:, 1, 2] = 2 * (y*z - r*x)\n",
    "    R[:, 2, 0] = 2 * (x*z - r*y)\n",
    "    R[:, 2, 1] = 2 * (y*z + r*x)\n",
    "    R[:, 2, 2] = 1 - 2 * (x*x + y*y)\n",
    "    return R\n",
    "\n",
    "\n",
    "def build_scaling_rotation(s, r):\n",
    "    L = np.zeros((s.shape[0], 3, 3))\n",
    "    R = build_rotation(r)\n",
    "\n",
    "    L[:,0,0] = s[:,0]\n",
    "    L[:,1,1] = s[:,1]\n",
    "    L[:,2,2] = s[:,2]\n",
    "\n",
    "    L = R @ L\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def homogeneous(points):\n",
    "    \"\"\"\n",
    "    homogeneous points\n",
    "    :param points: [..., 3]\n",
    "    \"\"\"\n",
    "    return np.column_stack((points, np.ones(points.shape[0])))\n",
    "\n",
    "def homogeneous_vec(vec, vectoadd = [0,0]):\n",
    "    \"\"\"\n",
    "    homogeneous points\n",
    "    :param points: [..., 3]\n",
    "    \"\"\"\n",
    "    return np.concatenate((vec,np.tile(vectoadd,(vec.shape[0],1,1))),axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getProjectionMatrix(znear, zfar, fovX, fovY):\n",
    "    import math\n",
    "    tanHalfFovY = math.tan((fovY / 2))\n",
    "    tanHalfFovX = math.tan((fovX / 2))\n",
    "\n",
    "    top = tanHalfFovY * znear\n",
    "    bottom = -top\n",
    "    right = tanHalfFovX * znear\n",
    "    left = -right\n",
    "\n",
    "    P = np.zeros((4, 4))\n",
    "\n",
    "    z_sign = 1.0\n",
    "\n",
    "    P[0, 0] = 2.0 * znear / (right - left)\n",
    "    P[1, 1] = 2.0 * znear / (top - bottom)\n",
    "    P[0, 2] = (right + left) / (right - left)\n",
    "    P[1, 2] = (top + bottom) / (top - bottom)\n",
    "    P[3, 2] = z_sign\n",
    "    P[2, 2] = z_sign * zfar / (zfar - znear)\n",
    "    P[2, 3] = -(zfar * znear) / (zfar - znear)\n",
    "    return P\n",
    "\n",
    "\n",
    "def focal2fov(focal, pixels):\n",
    "    import math\n",
    "    return 2*math.atan(pixels/(2*focal))\n",
    "\n",
    "\n",
    "def get_inputs(num_points=8):\n",
    "    length = 0.5\n",
    "    num_points = 8\n",
    "    x = np.linspace(-1, 1, num_points) * length\n",
    "    y = np.linspace(-1, 1, num_points) * length\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    means3D = np.stack([x,y, 0 * np.random.rand(*x.shape)], axis=-1).reshape(-1,3)\n",
    "    quats = np.tile(np.zeros((1,4)),(len(means3D), 1))\n",
    "    quats[..., 0] = 1.\n",
    "    scale = length /(num_points-1)\n",
    "    scales = np.tile(np.ones((1,3)),(len(means3D), 1))*scale\n",
    "    return means3D, scales, quats\n",
    "\n",
    "def get_cameras():    \n",
    "    intrins = np.array([[711.1111,   0.0000, 256.0000,   0.0000],\n",
    "               [  0.0000, 711.1111, 256.0000,   0.0000],\n",
    "               [  0.0000,   0.0000,   1.0000,   0.0000],\n",
    "               [  0.0000,   0.0000,   0.0000,   1.0000]])\n",
    "    c2w = np.array([[-8.6086e-01,  3.7950e-01, -3.3896e-01,  6.7791e-01],\n",
    "         [ 5.0884e-01,  6.4205e-01, -5.7346e-01,  1.1469e+00],\n",
    "         [ 1.0934e-08, -6.6614e-01, -7.4583e-01,  1.4917e+00],\n",
    "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]])\n",
    "\n",
    "    width, height = 512, 512\n",
    "    focal_x, focal_y = intrins[0, 0], intrins[1, 1]\n",
    "    viewmat = np.linalg.inv(c2w).T\n",
    "    FoVx = focal2fov(focal_x, width)\n",
    "    FoVy = focal2fov(focal_y, height)\n",
    "    projmat = getProjectionMatrix(znear=0.2, zfar=1000, fovX=FoVx, fovY=FoVy).T\n",
    "    projmat = viewmat @ projmat\n",
    "    return intrins, viewmat, projmat, height, width\n",
    "\n",
    "\n",
    "# Surface splatting (2D Gaussian Splatting)\n",
    "def setup(means3D, scales, quats, opacities, colors, viewmat, projmat):\n",
    "    rotations = build_scaling_rotation(scales, quats).permute(0,2,1)\n",
    "\n",
    "    # 1. Viewing transform\n",
    "    # Eq.4 and Eq.5\n",
    "    p_view = (means3D @ viewmat[:3,:3]) + viewmat[-1:,:3]\n",
    "    uv_view = (rotations @ viewmat[:3,:3])\n",
    "    M = torch.cat([homogeneous_vec(uv_view[:,:2,:]), homogeneous(p_view.unsqueeze(1))], dim=1)\n",
    "\n",
    "    T = M @ projmat # T stands for (WH)^T in Eq.9 \n",
    "    # 2. Compute AABB\n",
    "    # Homogneous plane is very useful for both ray-splat intersection and bounding box computation\n",
    "    # we know how to compute u,v given x,y homogeneous plane already; computing AABB is done by a reverse process.\n",
    "    # i.e compute the x, y s.t. \\|hu^4\\| = 1 and \\|h_v^4\\|=1 (distance of gaussian center to plane in the uv space)\n",
    "    temp_point = torch.tensor([[1.,1., -1.]]).cuda()\n",
    "    distance = (temp_point * (T[..., 3] * T[..., 3])).sum(dim=-1, keepdims=True)\n",
    "    f = (1 / distance) * temp_point\n",
    "    point_image = torch.cat(\n",
    "        [(f * T[..., 0] * T[...,3]).sum(dim=-1, keepdims=True),\n",
    "        (f * T[..., 1] * T[...,3]).sum(dim=-1, keepdims=True),\n",
    "        (f * T[..., 2] * T[...,3]).sum(dim=-1, keepdims=True)], dim=-1)\n",
    "\n",
    "    half_extend = point_image * point_image - torch.cat(\n",
    "        [(f * T[..., 0] * T[...,0]).sum(dim=-1, keepdims=True),\n",
    "        (f * T[..., 1] * T[...,1]).sum(dim=-1, keepdims=True),\n",
    "        (f * T[..., 2] * T[...,2]).sum(dim=-1, keepdims=True)], dim=-1)\n",
    "\n",
    "    radii = half_extend.clamp(min=1e-4).sqrt() * 3 # three sigma\n",
    "    center = point_image\n",
    "\n",
    "    # 3. Perform Sorting\n",
    "    depth = p_view[..., 2] # depth is used only for sorting\n",
    "    index = depth.sort()[1]\n",
    "    T = T[index]\n",
    "    colors = colors[index]\n",
    "    center = center[index]\n",
    "    depth = depth[index]\n",
    "    radii = radii[index]\n",
    "    opacities = opacities[index]\n",
    "    return T, colors, opacities, center, depth, radii\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make inputs\n",
    "import matplotlib\n",
    "num_points1=8\n",
    "means3d, scales, quats = get_inputs(num_points=num_points1)\n",
    "intrins, viewmat, projmat, height, width = get_cameras()\n",
    "\n",
    "\n",
    "intrins = intrins[:3,:3]\n",
    "colors = matplotlib.colormaps['Accent'](np.random.randint(1,64, 64)/64)[..., :3]\n",
    "\n",
    "opacities = np.ones(means3d[:,:1].shape)\n",
    "\n",
    "\n",
    "rotations = build_scaling_rotation(scales,quats) # sutu, svtv (scale * axis direction in gaussian FoR)\n",
    "projmat = frames['P1483CAM1.jpg'].projection\n",
    "\n",
    "\n",
    "# 1. Viewing transform\n",
    "# Eq.4 and Eq.5\n",
    "p_view = (means3d @ viewmat[:3,:3]) + viewmat[-1:,:3] # rotate the gaussian mean to camera FoR\n",
    "uv_view = (rotations @ viewmat[:3,:3]) # rotate to camera FoR\n",
    "\n",
    "# M is H matrix that representes the transformation from tangent plane to camera. \n",
    "# its the scaled axes concatenated to the gaussian mean location - represented in homogeneous coordinates\n",
    "\n",
    "# !! need to check that the order of axes ar ok for M !!\n",
    "M = np.concatenate((homogeneous_vec(uv_view[:,:,:2]),homogeneous(p_view)[:,:,np.newaxis]),axis = 2)\n",
    "M = np.moveaxis(M,[0,1,2],[0,2,1])\n",
    "T = M @ projmat # T stands for (WH)^T in Eq.9 - projmat transforms from camera to NDC (screen coordinates)\n",
    "# T is the transformation of every gaussian from tangent plane to NDC, its homogebnus coordinates. with the rotation matrix \n",
    "# representing the axes and the translation vector representing the location of the center of each gaussian. \n",
    "# We notice that projmat is a prespective projection matrix. \n",
    "\n",
    "# Next, We calculate the radius of the gaussian. We normalize by w to get homogeneus coordinates. In addition we flip Z axis (not sure why) \n",
    "# we calculate the distance from the camera to the gaussian mean (this is w, the last row of a homogenues coordinate, deviding by it will give perspective view)\n",
    "# Notice that the rotation is scaled (in build_scaling_rotation) and is not normalized.\n",
    "\n",
    "# point_image - the projectes mean of the gaussian (with flipped z)\n",
    "# half_extend - used to calculate the radius of the gaussian, we take 3 sigma. because the ratation is scaled \n",
    "# we calculate the distance for each axis and can get the 3 sigma by multiplying each distance. (we also devide by w to get the prespective view)\n",
    "\n",
    "temp_point = np.tile([1,1,-1],(T.shape[0],1))\n",
    "distance  = np.sum(temp_point*T[..., 3] * T[..., 3],1)\n",
    "f = (1 / distance[:,np.newaxis]) * temp_point\n",
    "\n",
    "# distance = (temp_point * (T[..., 3] * T[..., 3])).sum(dim=-1, keepdims=True)\n",
    "point_image = np.column_stack((np.sum(f * T[..., 0] * T[...,3],1),np.sum(f * T[..., 1] * T[...,3],1),np.sum(f * T[..., 2] * T[...,3],1)))\n",
    "\n",
    "axes_dist = np.column_stack((np.sum(f * T[..., 0] * T[...,0],1),np.sum(f * T[..., 1] * T[...,1],1),np.sum(f * T[..., 2] * T[...,2],1)))\n",
    "\n",
    "half_extend = point_image * point_image - axes_dist\n",
    "# distance = (temp_point * (T[..., 3] * T[..., 3])).sum(dim=-1, keepdims=True)\n",
    "# point_image = np.column_stack((np.sum(f * T[..., 0] * T[...,3],1),np.sum(f * T[..., 1] * T[...,3],1),np.sum(f * T[..., 2] * T[...,3],1)))\n",
    "\n",
    "    # half_extend = point_image * point_image - torch.cat(\n",
    "    #     [(f * T[..., 0] * T[...,0]).sum(dim=-1, keepdims=True),\n",
    "    #     (f * T[..., 1] * T[...,1]).sum(dim=-1, keepdims=True),\n",
    "    #     (f * T[..., 2] * T[...,2]).sum(dim=-1, keepdims=True)], dim=-1)\n",
    "\n",
    "    # radii = half_extend.clamp(min=1e-4).sqrt() * 3 # three sigma\n",
    "    # center = point_image\n",
    "\n",
    "# T, colors, opacities, center, depth, radii = setup(means3D, scales, quats, opacities, colors, viewmat, projmat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.54211958e+08, -1.73141762e+09, -1.52084014e+09],\n",
       "       [ 2.95378477e+08, -1.57838512e+09, -1.49386462e+09],\n",
       "       [-3.90517173e+08, -1.41906399e+09, -1.46578111e+09],\n",
       "       [-1.10517808e+09, -1.25305832e+09, -1.43651988e+09],\n",
       "       [-1.85045337e+09, -1.07993821e+09, -1.40600524e+09],\n",
       "       [-2.62835419e+09, -8.99236073e+08, -1.37415484e+09],\n",
       "       [-3.44107189e+09, -7.10442419e+08, -1.34087898e+09],\n",
       "       [-4.29099866e+09, -5.13001038e+08, -1.30607972e+09],\n",
       "       [ 1.39372973e+09, -1.46885891e+09, -1.54324478e+09],\n",
       "       [ 7.20834529e+08, -1.30495754e+09, -1.51578547e+09],\n",
       "       [ 1.93040686e+07, -1.13407805e+09, -1.48715769e+09],\n",
       "       [-7.12730112e+08, -9.55764905e+08, -1.45728519e+09],\n",
       "       [-1.47730272e+09, -7.69522008e+08, -1.42608496e+09],\n",
       "       [-2.27663365e+09, -5.74808097e+08, -1.39346641e+09],\n",
       "       [-3.11314951e+09, -3.71031477e+08, -1.35933054e+09],\n",
       "       [-3.98950832e+09, -1.57544002e+08, -1.32356889e+09],\n",
       "       [ 1.86469633e+09, -1.18751212e+09, -1.56725263e+09],\n",
       "       [ 1.17739143e+09, -1.01154088e+09, -1.53930883e+09],\n",
       "       [ 4.59745255e+08, -8.27797365e+08, -1.51013153e+09],\n",
       "       [-2.90297693e+08, -6.35754768e+08, -1.47963716e+09],\n",
       "       [-1.07498296e+09, -4.34837539e+08, -1.44773443e+09],\n",
       "       [-1.89676864e+09, -2.24415622e+08, -1.41432343e+09],\n",
       "       [-2.75835117e+09, -3.79783673e+06, -1.37929453e+09],\n",
       "       [-3.66269492e+09,  2.27775716e+08, -1.34252724e+09],\n",
       "       [ 2.37061401e+09, -8.85284809e+08, -1.59304222e+09],\n",
       "       [ 1.66859048e+09, -6.95858900e+08, -1.56461718e+09],\n",
       "       [ 9.34373032e+08, -4.97741266e+08, -1.53488870e+09],\n",
       "       [ 1.65693938e+08, -2.90319311e+08, -1.50376497e+09],\n",
       "       [-6.39932762e+08, -7.29214560e+07, -1.47114535e+09],\n",
       "       [-1.48523817e+09,  1.55190144e+08, -1.43691927e+09],\n",
       "       [-2.37322951e+09,  3.94827983e+08, -1.40096499e+09],\n",
       "       [-3.30722599e+09,  6.46888893e+08, -1.36314814e+09],\n",
       "       [ 2.91552535e+09, -5.59761673e+08, -1.62081967e+09],\n",
       "       [ 2.19853210e+09, -3.55275945e+08, -1.59192182e+09],\n",
       "       [ 1.44733067e+09, -1.41027998e+08, -1.56164535e+09],\n",
       "       [ 6.59411646e+08,  8.36987529e+07, -1.52988914e+09],\n",
       "       [-1.67986124e+08,  3.19692826e+08, -1.49654192e+09],\n",
       "       [-1.03790795e+09,  5.67823942e+08, -1.46148097e+09],\n",
       "       [-1.95372083e+09,  8.29053754e+08, -1.42457062e+09],\n",
       "       [-2.91915712e+09,  1.10444834e+09, -1.38566047e+09],\n",
       "       [ 3.50412175e+09, -2.08139758e+08, -1.65082414e+09],\n",
       "       [ 2.77199082e+09,  1.32769719e+07, -1.62146877e+09],\n",
       "       [ 2.00345956e+09,  2.45709811e+08, -1.59065404e+09],\n",
       "       [ 1.19574237e+09,  4.90002382e+08, -1.55826828e+09],\n",
       "       [ 3.45761788e+08,  7.47086763e+08, -1.52418813e+09],\n",
       "       [-5.49890766e+08,  1.01799540e+09, -1.48827694e+09],\n",
       "       [-1.49500084e+09,  1.30387500e+09, -1.45038296e+09],\n",
       "       [-2.49378454e+09,  1.60600280e+09, -1.41033718e+09],\n",
       "       [ 4.14187906e+09,  1.72852622e+08, -1.68333480e+09],\n",
       "       [ 3.39455982e+09,  4.13395095e+08, -1.65354625e+09],\n",
       "       [ 2.60845283e+09,  6.66432009e+08, -1.62221178e+09],\n",
       "       [ 1.78045577e+09,  9.32963495e+08, -1.58920775e+09],\n",
       "       [ 9.07126138e+08,  1.21409943e+09, -1.55439698e+09],\n",
       "       [-1.53666836e+07,  1.51107494e+09, -1.51762680e+09],\n",
       "       [-9.91297757e+08,  1.82526855e+09, -1.47872687e+09],\n",
       "       [-2.02545304e+09,  2.15822370e+09, -1.43750646e+09],\n",
       "       [ 4.83522885e+09,  5.87058209e+08, -1.71867955e+09],\n",
       "       [ 4.07283432e+09,  8.49317853e+08, -1.68849413e+09],\n",
       "       [ 3.26905141e+09,  1.12582731e+09, -1.65667025e+09],\n",
       "       [ 2.42041318e+09,  1.41778130e+09, -1.62307067e+09],\n",
       "       [ 1.52305406e+09,  1.72651203e+09, -1.58754238e+09],\n",
       "       [ 5.72650761e+08,  2.05350961e+09, -1.54991427e+09],\n",
       "       [-4.35647544e+08,  2.40044615e+09, -1.50999434e+09],\n",
       "       [-1.50730234e+09,  2.76920448e+09, -1.46756642e+09]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'clamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 54\u001b[0m\n\u001b[0;32m     50\u001b[0m axes_dist \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack((np\u001b[38;5;241m.\u001b[39msum(f \u001b[38;5;241m*\u001b[39m T[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m T[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m),np\u001b[38;5;241m.\u001b[39msum(f \u001b[38;5;241m*\u001b[39m T[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m T[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m1\u001b[39m),np\u001b[38;5;241m.\u001b[39msum(f \u001b[38;5;241m*\u001b[39m T[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m T[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;241m2\u001b[39m],\u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m     52\u001b[0m half_extend \u001b[38;5;241m=\u001b[39m point_image \u001b[38;5;241m*\u001b[39m point_image \u001b[38;5;241m-\u001b[39m axes_dist\n\u001b[1;32m---> 54\u001b[0m radii \u001b[38;5;241m=\u001b[39m \u001b[43mhalf_extend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;66;03m# three sigma\u001b[39;00m\n\u001b[0;32m     55\u001b[0m center \u001b[38;5;241m=\u001b[39m point_image\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'clamp'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_file = \"I:/My Drive/Research/gs_data/mov19_2022_03_03/2dgs_output/84444b22-6/point_cloud/iteration_1000/point_cloud.ply\"\n",
    "vertices = PlyData.read(input_file)[\"vertex\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "means3d = np.column_stack((vertices['x'],vertices['y'],vertices['z']))\n",
    "viewmat = frames['P1483CAM1.jpg'].world_to_cam\n",
    "\n",
    "\n",
    "\n",
    "r = np.column_stack([(vertices[f'rot_{idx}']) for idx in range(4)])\n",
    "s = np.column_stack(([(vertices[f'scale_{idx}']) for idx in range(2)]))\n",
    "s = np.column_stack((s,vertices['scale_0']))\n",
    "rotations = build_scaling_rotation(s,r) # sutu, svtv (scale * axis direction in gaussian FoR)\n",
    "projmat = frames['P1483CAM1.jpg'].projection\n",
    "\n",
    "\n",
    "# 1. Viewing transform\n",
    "# Eq.4 and Eq.5\n",
    "p_view = (means3d @ viewmat[:3,:3]) + viewmat[-1:,:3] # rotate the gaussian mean to camera FoR\n",
    "uv_view = (rotations @ viewmat[:3,:3]) # rotate to camera FoR\n",
    "\n",
    "# M is H matrix that representes the transformation from tangent plane to camera. \n",
    "# its the scaled axes concatenated to the gaussian mean location - represented in homogeneous coordinates\n",
    "\n",
    "# !! need to check that the order of axes ar ok for M !!\n",
    "M = np.concatenate((homogeneous_vec(uv_view[:,:,:2]),homogeneous(p_view)[:,:,np.newaxis]),axis = 2)\n",
    "M = np.moveaxis(M,[0,1,2],[0,2,1])\n",
    "T = M @ projmat # T stands for (WH)^T in Eq.9 - projmat transforms from camera to NDC (screen coordinates)\n",
    "# T is the transformation of every gaussian from tangent plane to NDC, its homogebnus coordinates. with the rotation matrix \n",
    "# representing the axes and the translation vector representing the location of the center of each gaussian. \n",
    "# We notice that projmat is a prespective projection matrix. \n",
    "\n",
    "# Next, We calculate the radius of the gaussian. We normalize by w to get homogeneus coordinates. In addition we flip Z axis (not sure why) \n",
    "# we calculate the distance from the camera to the gaussian mean (this is w, the last row of a homogenues coordinate, deviding by it will give perspective view)\n",
    "# Notice that the rotation is scaled (in build_scaling_rotation) and is not normalized.\n",
    "\n",
    "# point_image - the projectes mean of the gaussian (with flipped z)\n",
    "# half_extend - used to calculate the radius of the gaussian, we take 3 sigma. because the ratation is scaled \n",
    "# we calculate the distance for each axis and can get the 3 sigma by multiplying each distance. (we also devide by w to get the prespective view)\n",
    "temp_point = np.tile([1,1,-1],(T.shape[0],1))\n",
    "distance  = np.sum(temp_point*T[..., 3] * T[..., 3],1)\n",
    "f = (1 / distance[:,np.newaxis]) * temp_point\n",
    "\n",
    "# distance = (temp_point * (T[..., 3] * T[..., 3])).sum(dim=-1, keepdims=True)\n",
    "point_image = np.column_stack((np.sum(f * T[..., 0] * T[...,3],1),np.sum(f * T[..., 1] * T[...,3],1),np.sum(f * T[..., 2] * T[...,3],1)))\n",
    "\n",
    "axes_dist = np.column_stack((np.sum(f * T[..., 0] * T[...,0],1),np.sum(f * T[..., 1] * T[...,1],1),np.sum(f * T[..., 2] * T[...,2],1)))\n",
    "\n",
    "half_extend = point_image * point_image - axes_dist\n",
    "\n",
    "radii = half_extend.clamp(min=1e-4).sqrt() * 3 # three sigma\n",
    "center = point_image\n",
    "    # half_extend = point_image * point_image - torch.cat(\n",
    "    #     [(f * T[..., 0] * T[...,0]).sum(dim=-1, keepdims=True),\n",
    "    #     (f * T[..., 1] * T[...,1]).sum(dim=-1, keepdims=True),\n",
    "    #     (f * T[..., 2] * T[...,2]).sum(dim=-1, keepdims=True)], dim=-1)\n",
    "\n",
    "    # radii = half_extend.clamp(min=1e-4).sqrt() * 3 # three sigma\n",
    "    # center = point_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roni\\AppData\\Local\\Temp\\ipykernel_25052\\332239304.py:1: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in sqrt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[           nan,            nan, 1.24115344e+08],\n",
       "       [           nan,            nan,            nan],\n",
       "       [           nan,            nan,            nan],\n",
       "       ...,\n",
       "       [           nan,            nan, 1.29938590e+08],\n",
       "       [           nan,            nan,            nan],\n",
       "       [           nan,            nan, 1.57365218e+08]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(half_extend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6079, 4, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surface splatting (2D Gaussian Splatting)\n",
    "def setup(means3D, scales, quats, opacities, colors, viewmat, projmat):\n",
    "    rotations = build_scaling_rotation(scales, quats).permute(0,2,1)\n",
    "\n",
    "    # 1. Viewing transform\n",
    "    # Eq.4 and Eq.5\n",
    "    p_view = (means3D @ viewmat[:3,:3]) + viewmat[-1:,:3]\n",
    "    uv_view = (rotations @ viewmat[:3,:3])\n",
    "    M = torch.cat([homogeneous_vec(uv_view[:,:2,:]), homogeneous(p_view.unsqueeze(1))], dim=1)\n",
    "\n",
    "    T = M @ projmat # T stands for (WH)^T in Eq.9\n",
    "    # 2. Compute AABB\n",
    "    # Homogneous plane is very useful for both ray-splat intersection and bounding box computation\n",
    "    # we know how to compute u,v given x,y homogeneous plane already; computing AABB is done by a reverse process.\n",
    "    # i.e compute the x, y s.t. \\|hu^4\\| = 1 and \\|h_v^4\\|=1 (distance of gaussian center to plane in the uv space)\n",
    "    temp_point = torch.tensor([[1.,1., -1.]]).cuda()\n",
    "    distance = (temp_point * (T[..., 3] * T[..., 3])).sum(dim=-1, keepdims=True)\n",
    "    f = (1 / distance) * temp_point\n",
    "    point_image = torch.cat(\n",
    "        [(f * T[..., 0] * T[...,3]).sum(dim=-1, keepdims=True),\n",
    "        (f * T[..., 1] * T[...,3]).sum(dim=-1, keepdims=True),\n",
    "        (f * T[..., 2] * T[...,3]).sum(dim=-1, keepdims=True)], dim=-1)\n",
    "\n",
    "    half_extend = point_image * point_image - torch.cat(\n",
    "        [(f * T[..., 0] * T[...,0]).sum(dim=-1, keepdims=True),\n",
    "        (f * T[..., 1] * T[...,1]).sum(dim=-1, keepdims=True),\n",
    "        (f * T[..., 2] * T[...,2]).sum(dim=-1, keepdims=True)], dim=-1)\n",
    "\n",
    "    radii = half_extend.clamp(min=1e-4).sqrt() * 3 # three sigma\n",
    "    center = point_image\n",
    "\n",
    "    # 3. Perform Sorting\n",
    "    depth = p_view[..., 2] # depth is used only for sorting\n",
    "    index = depth.sort()[1]\n",
    "    T = T[index]\n",
    "    colors = colors[index]\n",
    "    center = center[index]\n",
    "    depth = depth[index]\n",
    "    radii = radii[index]\n",
    "    opacities = opacities[index]\n",
    "    return T, colors, opacities, center, depth, radii\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2DGS - Math explained\n",
    "### Modeling: \n",
    "The 2D Gaussian is defined in terms of its local coordinate system, where the $ X$ and $ Y$ axes are scaled according to the Gaussian's shape. The Gaussian is represented in the **world coordinates**, with the **axes** of the Gaussian in world space defined by the tangential vectors $ \\mathbf{t_u} $ and $ \\mathbf{t_v} $, and its **scaling factors** by $ s_u $ and $ s_v $. These tangential vectors define the directions of the local coordinate axes in the tangent plane (the object FoR).\n",
    "\n",
    "The **object plane**, which is the tangent plane to the Gaussian in world space, can be described using the plane equation:\n",
    "$$\n",
    "P(u, v) = p_k + s_u \\mathbf{t_u} u + s_v \\mathbf{t_v} v\n",
    "$$\n",
    "where $ p_k $ is the center of the Gaussian in world coordinates, and $ u $ and $ v $ represent local coordinates on the tangent plane. This equation expresses the position of any point in the tangent plane in terms of the world coordinates, as modified by the scaling along the tangential axes $ \\mathbf{t_u} $ and $ \\mathbf{t_v} $.\n",
    "\n",
    "To transform the local coordinates in the tangent plane (object frame of reference) to world coordinates, we define the following transformation matrix $ H $, which encodes the **scaling**, **rotation**, and **translation** from the object space to the world space:\n",
    "$$\n",
    "H = \\begin{pmatrix}\n",
    "s_u \\mathbf{t_u} & s_v \\mathbf{t_v} & 0 & p_k \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "R & p_k \\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "where:\n",
    "- $ \\mathbf{t_u} = \\begin{pmatrix} t_{u_x} \\\\ t_{u_y} \\\\ t_{u_z} \\end{pmatrix} $ and $ \\mathbf{t_v} = \\begin{pmatrix} t_{v_x} \\\\ t_{v_y} \\\\ t_{v_z} \\end{pmatrix} $ are the 3D tangential vectors (defining the axes of the tangent plane in world coordinates),\n",
    "- $ p_k = \\begin{pmatrix} p_{k_x} \\\\ p_{k_y} \\\\ p_{k_z} \\end{pmatrix} $ is the 3D position of the Gaussian center in world coordinates, and\n",
    "- $ R $ is the 3x3 rotation matrix that describes the orientation of the tangent plane in world space.\n",
    "\n",
    "The matrix $ H $ can be interpreted in two parts:\n",
    "- The first part, $ \\begin{pmatrix} s_u \\mathbf{t_u} & s_v \\mathbf{t_v} & 0 \\end{pmatrix} $, represents the scaling and rotation of the axes in the world coordinates. It scales the tangential vectors $ \\mathbf{t_u} $ and $ \\mathbf{t_v} $ by $ s_u $ and $ s_v $, respectively, and applies any rotational transformation.\n",
    "- The second part, $ p_k $, represents the **translation** of the Gaussian's center in world coordinates, which shifts the origin of the local tangent plane to the desired world position.\n",
    "\n",
    "The matrix can also be interpreted in terms of the rotation matrix $ R $ (which describes the orientation of the tangent plane in world space) and the translation vector $ p_k $. The transformation from local coordinates $ (u, v) $ to world coordinates $ (x, y, z) $ is thus achieved through the multiplication of $ H $ by the local coordinate vector:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "= H \\begin{pmatrix}\n",
    "u \\\\\n",
    "v \\\\\n",
    "1 \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This transformation allows us to map a point in the tangent plane (object space) to the corresponding point in the world space, accounting for both the Gaussian's shape (through the scaling $ s_u, s_v $) and its orientation (through the rotation $ R $).\n",
    "\n",
    "In summary, the matrix $ H $ serves as a **homogeneous transformation** that encapsulates both the geometric properties (scaling and rotation) and the positioning (translation) of the 2D Gaussian in world space.\n",
    "\n",
    "for every u,v on the tangent plane we can calculate the gaussian power: \n",
    "$$ G(u) = \\exp\\left( -\\frac{u^2 + v^2}{2} \\right) \\tag{6} $$\n",
    "\n",
    "* $ \\mathbf{t_u} $ , $ \\mathbf{t_v} , $ $\\mathbf{s_u} $ and $\\mathbf{s_v}$ are learnable parameters.\n",
    "* each gaussian is defined by the opacity $\\alpha$ and a view dependent color.\n",
    "\n",
    "### Splatting\n",
    "\n",
    "\n",
    "The goal of this process is to find the intersection between a ray, originating from a pixel in the image, and the tangent plane of a 2D Gaussian, and then evaluate the power of the Gaussian at the intersection point. Below are the detailed steps.\n",
    "\n",
    "Step 1: Defining the Image Ray \n",
    "\n",
    "We begin by defining the ray from a pixel in the image using two orthogonal planes. The pixel location $(x, y) $ in the image space can be used to define two planes:\n",
    "\n",
    "\n",
    "* The x-plane (yz): a plane defined by a normal vector $ \\mathbf{n}_x = (-1, 0, 0) $ and an offset $ x $. The 4D homogeneous form is $ h_x = (-1, 0, 0, x) $.\n",
    "* The y-plane (xz): a plane defined by a normal vector $ \\mathbf{n}_y = (0, -1, 0) $ and an offset $ y $. The 4D homogeneous form is $ h_y = (0, -1, 0, y) $.\n",
    "\n",
    "\n",
    "These planes intersect at a ray in 3D space that is represented as the line of intersection between the two planes.\n",
    "\n",
    "Step 2: Transforming the Planes to the Tangent Plane\n",
    "\n",
    "Next, we transform the planes from the image space into the local coordinates of the 2D Gaussian primitive (the tangent plane). Using the transformation matrix $ M = (WH)^{-1} $, we apply the inverse transpose to the planes (W is the prespective projection matrix (transformation from camera to screen space), H is a transformation from tangant plane to camera space):\n",
    "\n",
    "multiplying W (camera to screen) by H (tangent plane to camera) will transform a coordinate from tangent plane to camera to screen. Transposing it will map from screen space to tangent plane \n",
    "\n",
    "$$\n",
    "h_u = (WH)^\\top h_x \\\\\n",
    "h_v = (WH)^\\top h_y\n",
    "$$\n",
    "* those are the planes rotated from image space to tangent space\n",
    "\n",
    "Step 3: Solving for the Intersection\n",
    "\n",
    "The next step is to solve for the intersection point $(u, v) $ of the ray with the transformed tangent plane. We do this by solving the following system of equations:\n",
    "\n",
    "$$\n",
    "h_u \\cdot (u, v, 1, 1)^\\top = 0 \\\\\n",
    "h_v \\cdot (u, v, 1, 1)^\\top = 0\n",
    "$$\n",
    "\n",
    "Expanding these equations gives us:\n",
    "\n",
    "\n",
    "$$\n",
    "    h_1^u u + h_2^u v + h_3^u + h_4^u = 0 \\\\\n",
    "    h_1^v u + h_2^v v + h_3^v + h_4^v = 0\n",
    "$$\n",
    "\n",
    "\n",
    "The solution for $ u $ and $ v $ is obtained by solving this system of equations. This leads to the following expressions for the coordinates $ u(x) $ and $ v(x) $:\n",
    "\n",
    "$$\n",
    "u = \\frac{h_2^u h_4^v - h_4^u h_2^v}{h_1^u h_2^v - h_2^u h_1^v} \\\\\n",
    "v = \\frac{h_4^u h_1^v - h_1^u h_4^v}{h_1^u h_2^v - h_2^u h_1^v}\n",
    "$$\n",
    "\n",
    "Where $ h_i^u $ and $ h_i^v $ are the homogeneous parameters of the transformed planes.\n",
    "\n",
    "Step 4: Evaluating the Gaussian at the Intersection\n",
    "\n",
    "Once we have the coordinates $ (u, v) $, we can evaluate the 2D Gaussian function at the intersection point. The Gaussian function is typically of the form:\n",
    "\n",
    "$$\n",
    "G(u, v) = \\exp\\left( -\\frac{u^2 + v^2}{2} \\right)\n",
    "$$\n",
    "\n",
    "This represents the Gaussian value at the point $ (u, v) $ on the tangent plane.\n",
    "\n",
    "\n",
    "Once the values of $ u(x) $ and $ v(x) $ are found, we can compute the depth of the intersection point using the following equation:\n",
    "$$\n",
    "x = (x_z, y_z, z, z)^\\top = W P(u, v) = W H (u, v, 1, 1)^\\top\n",
    "$$\n",
    "Here, $ W $ is the camera projection matrix, and $ H $ is the transformation matrix from the tangent plane to world coordinates. The last component of the resulting vector $ x = (xz, yz, z, z)^\\top $ gives the depth $ z $ of the intersection point (the third element, need to make sure). \n",
    "\n",
    "\n",
    "\n",
    "#### Screen space\n",
    "this transformation returns a 3d coordinate in screen space. \n",
    ", where rendering occures. the projection matrix transformes x,y,z to the screen space\n",
    "* x axis spans from 0 to -1 (width)\n",
    "* y axis spans from 0 to -1 (hight)\n",
    "* Z_{NDC} represents the depth (need to check that)\n",
    "from NDC we can map to pixels\n",
    "\n",
    "#### Transforming planes\n",
    "* A point in 3D homogeneous coordinates is represented as $ (x,y,z,w)\n",
    "* a plane consist of both a normal and an offset - in 3D homogeneous coordinates it is represented as $(a,b,c,d)\n",
    "where $(a,b,c) is the planes normal vector and $d$ is its offset. \n",
    "\n",
    "The plane equation : $ ax + by + cz + dw = 0 $\n",
    "which means that that dot product of the plane parameters $ (a,b,c,d)$ and a point $(x,y,z,w)$ is zero for a point lying on the plane. \n",
    "\n",
    "** Transforming Point on a Plane **\n",
    "we Define matrix $M$ - a transformation matrix. \n",
    "tansforming a point on the plain using $M$: $p'=M\\cdot p$\n",
    "after transforming it, the plane equation should still hold: $h'p' = 0$ were $h', p'$ are the transformed plane and point parameters.\n",
    "\n",
    "Because $p' = M\\cdot p$ we get that $h'(M\\cdot p) = 0 $ and since $ h' \\cdot(M\\cdotp p)=M^T \\cdot h' \\cdot p $ we can write \n",
    "$$\n",
    "M^T \\cdot h' \\cdot p = h\\cdot p \\\\\n",
    "M^T \\cdot h' = h\n",
    "$$\n",
    "\n",
    "and from that we show that the plane $h'$ remains valid after transformation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
